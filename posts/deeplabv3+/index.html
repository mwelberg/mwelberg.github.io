<!DOCTYPE html>
<html lang="de">
    <head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Deeplabv3&#43; &middot; moritzwelberg.de</title>

		
  		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/fonts.css">
		
		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

		
		<link href="" rel="alternate" type="application/rss+xml" title="moritzwelberg.de" />
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="/">
					<h2 class="nav-title">moritzwelberg.de</h2>
				</a>
				<ul>
    
    
        <li>
            <a href="/about">
                
                <span>About</span>
                
            </a>
        </li>
    
</ul>
			</div>
		</nav>

        

<main>
	<div class="post">
		<div class="post-info">
        <br>
        <time datetime="2020-10-14 21:12:10 &#43;0200 CEST">14. Oktober 2020</time>
</div>

		<h1 class="post-title">Deeplabv3&#43;</h1>
<div class="post-line"></div>

		

		

<h1 id="einleitung">Einleitung</h1>

<p>Die Segmentierung von Bildern in unterschiedliche Bildbereiche, aufgrund
der Bedeutung der Bildinhalte ist ein Problem, welches derzeit in vielen
Anwendungsbereichen, wie z.B. der Medizinischen Bildgebung oder der
Automobilindustrie erforscht wird[1, 2].<br />
Das Problem der semantischen Bildsegmentierung lässt sich in zwei
untergeordnete Aufgaben unterteilen. Zum Einen die Aufgabe der
Objekterkennung, welche eine generelle Trennung von unterschiedlichen
Bildsegmenten bezüglich derer Bedeutung ermöglicht. Zum Anderen die
Optimierung der Grenzschärfe. Hierbei geht es darum möglichst genaue
Grenzen zwischen einzelnen, semantisch unabhängigen Bildsegmenten zu
finden.</p>

<h1 id="grundlegende-arbeiten">Grundlegende Arbeiten</h1>

<p>Bekannte Architekturen neuronaler Netze zur semantischen
Bildsegmentierung wie PSPNet und ResNet-38, liefern bisweilen sehr gute
Ergebnisse bei der Erkennung von Objekten in
Bildern[3, 4]. Allerdings erreichen diese Modelle eher
niedrige Werte bei der mean intersection over union (mIOU). Dieser Wert
gibt an, wie genau die gefundenen Segmente eines segmentierten
Datensatzes von Bildern mit den tatsächlichen Bildsegmenten überlappen.<br />
Neben der Objekterkennung liegt der Fokus von DeepLabv3 auch auf einer
Maximierung der mIOU.</p>

<h2 id="atrous-convolution">Atrous Convolution</h2>

<p>Während die Objekterkennung z.B. im PSPNet hauptsächlich über pyramid
pooling gewährleistet wird, kommt bei DeepLabv3 Atrous Convolution in
Kombination mit pyramid pooling zum Einsatz[3,5].<br />
Bei beiden Methoden handelt es sich um Wege, wie man räumliche
Informationen von Bildobjekten im Verlauf der feature extraction eines
CNNs beibehalten kann. Der Trick zum Erhalt von räumlicher Information
bei beiden Methoden ist die Änderung der Kernelgröße bei mehreren
Faltungsoperationen auf denselben Daten.<br />
Beim pyramid pooling verwendet man Kernel-Masken, die jeden Pixel
innerhalb der Maske in die Poolingoperation mit einbeziehen. Bei einem
$3\times3$ Kernel, wären das also $9$ Werte, die in die Poolingoperation
eingehen. Diese Masken werden beim pyramid pooling in unterschiedlicher
Größe auf die gleichen Daten angewandt. So erhält man im Gegensatz zu
herkömmlichen Poolingoperationen mehr Informationen über räumliche
Zusammenhänge in den Daten.</p>

<p><img src="../vortrag/img/atrous_spatial_pyramid_pooling_no_caption" alt="image" />{width=&rdquo;\textwidth&rdquo;
height=&ldquo;120pt&rdquo;}</p>

<p></p>

<p>[[fig:atrousConv]]{#fig:atrousConv label=&ldquo;fig:atrousConv&rdquo;} <img src="../vortrag/img/fig-3-depthwise-and-atrous-depthwise-conv" alt="Depthwise
Atrous
Convolution" title="fig:" /></p>

<p><br />
Bei Atrous Convolution werden zwischen den, für die Poolingoperation zu
erfassenden Pixelwerten Lücken eingebaut. Bei einem $3\times3$ Kernel
könnten mit einer Rate von $2$ beispielsweise eine Fläche von $5\times5$
Pixeln abgedeckt werden, da die Poolingoperation nur auf jeden zweiten
Pixel angewandt wird, dabei jedoch die ursprüngliche Kernelgröße von
$3\times3$ abgefragten Werten beibehalten wird (siehe
Fig.<a href="#fig:atrousConv">[fig:atrousConv]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;fig:atrousConv&rdquo;}&copy;). So erhält man Feature Maps, die
Informationen über Flächenzusammenhänge beliebiger größe beinhalten,
jedoch mit der gleichen Anzahl an Operationen wie herkömmliches Pyramid
Pooling berechenbar sind.</p>

<h2 id="depthwise-separable-convolution">Depthwise Separable Convolution</h2>

<p>Die Effizienz einer Faltung auf Bilddaten mit mehreren Tiefenkanälen
kann mit Hilfe von Depthwise Separable Convolution noch verbessert
werden.<br />
Bei der Depthwise Separable Convolution (DSC) handelt es sich um ein
Prinzip, welches bei der Faltung von Daten mit mehreren Tiefenkanälen
angewandt werden kann, um die Berechnung der Features zu
beschleunigen[@mobileNet].<br />
Bei einer herkömmlichen Faltung auf $d \times d$ großen Daten mit $t$
Tiefenkanälen und einer Kernelgröße von $k \times k$, benötigt man unter
Verwendung von $n$ Kerneln also $d^2 \times t \times k^2 \times n$
Multiplikationen zur Errechnung der Feature Map eines Bildes.<br />
Bei der DSC wird der Faltungsprozess in zwei Schritte unterteilt.<br />
Zunächst wird tiefenweise pro Kanal $d^2 \times k^2$ Mal gefaltet. Das
entspricht, unter Berücksichtigung aller Kanäle einem Produkt von
$d^2 \times k^2 \times t$.<br />
Im zweiten Schritt wird nun eine punktweise $1\times1$ Faltung über alle
Kanäle zur Konkatenation der Ergebnisse vorgenommen. Für jeden Kernel
$n$ ergibt sich also ein Produkt $d^2 \times t$.<br />
Insgesamt ergibt sich für die DSC also ein Rechenaufwand von
$d^2 \times k^2 \times t + d^2 \times t \times n$ bzw.
$t \times d^2 (k^2 + n)$ Multiplikationen.<br />
Um zu berechnen wie stark die DSC den Rechenaufwand im Vergleich zur
herkömmlichen Faltung reduziert genügt es die Produkte zu vergleichen:<br />
$$\frac{t \times d^2 (k^2 + n)}{d^2 \times t \times k^2 \times n}
=\frac{k^2+n}{k^2\times n}
=\frac{1}{n} + \frac{1}{k^2}$$ Dieser Vergleich zeigt, je größer die
Anzahl oder die Größe der Kernel, umso kleiner die Differenz zwischen
herkömmlicher Faltung und DSC.</p>

<h2 id="deeplabv3-network-backbone">DeepLabv3 Network Backbone</h2>

<p>Als Unterbau dient in DeepLabv3 das Xception CNN Modell[@xception].
Wobei in diesem Anwendungsfall die, in Xception vorgesehenen max-pooling
Operationen durch depthwise atrous separable convolution ersetzt wurden.
Das Xception Modell hat sich im praktischen Test gegenüber dem
ResNet-101 als geeigneter für die Objekterkennung und somit zur
Kodierung der Bildelemente in Feature Vektoren erwiesen. ResNet-101
hatte hier einen Top-1 Fehler von 22.40%, wohingegen das modifizierte
Xception Modell 20.19% erreichte[@encoderDecoder].<br />
Das verwendete CNN besteht aus mehreren, sequenziell ausgeführten
Blöcken ähnlicher Faltungs- und Poolingoperationen. In einer
vorangegangenen Arbeit wurde vorgeschlagen den letzten dieser Blöcke
wiederholt anzuwenden und dabei über Atrous Convolution den gewünschten
ouput_stride festzulegen[@rethinking]. Der output_stride gibt das
Verhältnis vom Eingangsbild zur Ausgangs Featuremap an. Ein
output_stride von 16 bedeutet, dass die Featuremap 16 mal kleiner ist,
als das ursprüngliche Bild. Ohne die Konfigurationsmöglichkeit dieses
Parameters würde bei wiederholter Anwendung des letzten Blocks der
output_stride exponentiell anwachsen, was negative Auswirkungen auf die
Rekonstruktion der Bildsegmente aus den Featuremaps hat[@rethinking].
Fig. <a href="#fig:backbone">[fig:backbone]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;fig:backbone&rdquo;} illustriert die Anwendung dieses Prinzips in
Kombination mit einem abschließenden Atrous Spatial Pyramid Pooling.</p>

<p><img src="../vortrag/img/fig-1-encoder-decoder-with-atrous-conv" alt="image" />{width=&rdquo;\textwidth&rdquo;
height=&ldquo;180pt&rdquo;}</p>

<h1 id="methodik-sec-methodik">Methodik {#sec:methodik}</h1>

<p>Um die bestehende DeepLabv3 Architektur hinsichtlich der Grenzschärfe zu
verbessern, wird der bisherige Aufbau um eine Encoder-Decoder Struktur
ergänzt. Dabei handelt es sich um ein bewährtes Prinzip zur Optimierung
der mIOU[@segnet]. Wird bei einem Schritt der Feature Extraction im
Netzwerk das Bild durch Faltung oder pooling herunterskaliert, so gibt
es bei der Rekonstruktion der Bildinformationen aus den Features einen
korrespondierenden Schritt, der das Bild wieder hochskaliert.<br />
Bei DeepLabv3+ wird das bisherige DeepLabv3 Modell als Kodierer
eingesetzt. Hierbei wird zusätzlich zum Xception Modell mit Atrous
Convolution noch eine $1\times1$ Faltung zur Reduktion der, aus der
Atrous Convolution resultierenden Kanäle angewendet.<br />
Das Modell, sowie das implementierte Framework[^1] lassen im Rahmen der
Atrous Convolution eine Anpassung an die vorhandene Rechenkapazität zu.
Die Poolingoperationen können über die Parameter der Atrous Convolution
(Abstände, Schrittgröße, etc.) konfiguriert und auf den Anwendungsfall
zugeschnitten werden. Die Qualität der, aus dem neuronalen Netz
erzeugten Features wird ebenfalls direkt über diese Parameter
beeinflusst.<br />
Der Dekodierer (siehe Fig.
<a href="#fig:deeplabv3+">[fig:deeplabv3+]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;fig:deeplabv3+&ldquo;}) erhält eingangs die low-level features aus
dem neronalen Netz und konkateniert diese mit dem 4-fach hochskalierten
Ergebnis der $1\times1$ Faltung. Nach einer weiteren Faltungs- und
Skalierungsoperation, erhält man so eine Vorhersage mit vergleichsweise
hoher IOU Abdeckung.<br />
In Fig. <a href="#fig:deeplabv3+">[fig:deeplabv3+]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;fig:deeplabv3+&ldquo;} wird dargestellt, dass der Decoder bereits
mit wenigen Operationen gute Ergebnisse erzielt.</p>

<h1 id="evaluation-sec-eval">Evaluation {#sec:eval}</h1>

<p>Im experimentellen Benchmarking auf den PASCAL VOC 2012 und Cityscapes
Datensätzen erreicht DeepLabv3+ eine mIOU von 89.0 und 82.1 respektive.
Dies beschreibt eine Verbesserung um 2.1% bzw. 0.8% gegenüber DeepLabv3.
Auf dem Cityscapes Datensatz (siehe
Fig.<a href="#fig:cityEval">[fig:cityEval]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;fig:cityEval&rdquo;}) schlägt DeepLabv3+ damit knapp andere
state-of-the-art Modelle wie Mapillary um 0.1%, PSPNet um 0.9% und
RestNet-38 um 1.5%.</p>

<p>\centering
  <strong>Methode</strong>                    <strong>mIOU</strong></p>

<hr />

<p>ResNet-38[@resNet38]             80.6
  PSPNet[@pspnet]                  81.2
  Mapillary[@mapillary]            82.0
  DeepLabv3[@rethinking]           81.3
  DeepLabv3+[@encoderDecoder]      82.1</p>

<p>Auf PASCAL VOC 2012 ist der Abstand zu den anderen Modellen
vergleichsweise groß. Hier schlägt DeepLabv3+ die bisher beste Methode
um 2.2%.</p>

<p>\centering
  <strong>Methode</strong>                                   <strong>mIOU</strong></p>

<hr />

<p>Deep Layer Cascade[@deepLayerCascade]           82.7
  ResNet-38_MS_COCO[@resNet38]                  84.9
  PSPNet[@pspnet]                                 85.4
  DIS[@dis]                                       86.8
  DeepLabv3[@rethinking]                          85.7
  DeepLabv3-JFT[@rethinking]                      86.9
  DeepLabv3+ (Xception)[@encoderDecoder]          87.8
  DeepLabv3+ (Xception-JFT)[@encoderDecoder]      89.0</p>

<p></p>

<p><img src="../vortrag/img/fig-2-example-image-with-schema" alt="image" />{width=&rdquo;\textwidth&rdquo;
height=&ldquo;200pt&rdquo;}</p>

<p>In Fig. <a href="#pascalEval">[pascalEval]</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;pascalEval&rdquo;} wird ersichtlich, wie DeepLabv3+ im Vergleich zu
anderen state-of-the-art Modellen auf dem Benchmarkdatensatz PASCAL VOC
2012 abschneidet.</p>

<h1 id="ausblick-sec-ausblick">Ausblick {#sec:ausblick}</h1>

<p>Durch die Arbeiten von L. Chen, Y. Zhu, G. Papandreou, F. Schroff und H.
Adam im Rahmen des DeepLab Modells gibt es mit DeepLabv3+ eine
nachweislich konkurrenzfähige Alternative zu bereits etablierten
Modellen der semantischen Bildsegmentierung.<br />
Mit hilfe vergleichbarer Benchmarks auf öffentlichen Datensätzen konnte
gezeigt werden, dass DeepLabv3+ dank der Encoder-Decoder Struktur einen
neuen Maßstab beim Erreichen großer Grenzschärfe (mIOU) setzt.<br />
In Zukunft könnte DeepLabv3+ noch hinsichtlich folgender
Problemstellungen verbessert werden[@encoderDecoder]:</p>

<ul>
<li><p>Unterscheidung sehr ähnlicher Objekte (z.B. Couch und Sessel)</p></li>

<li><p>Segmentierung stark verdeckter Objekte</p></li>

<li><p>Erkennung von Objekten aus ungewöhnlichen Blickwinkeln</p></li>
</ul>

<p>\bibliographystyle{IEEEbib}</p>

<p>[*] <a href="https://github.com/tensorflow/models/tree/master/research/deeplab">https://github.com/tensorflow/models/tree/master/research/deeplab</a> <br />
[1] Mohammad Havaei, Axel Davy, David Warde-Farley,
Antoine Biard, Aaron Courville, Yoshua Bengio, Chris
Pal, Pierre Marc Jodoin, and Hugo Larochelle, “Brain
tumor segmentation with Deep Neural Networks,” Med-
ical Image Analysis, vol. 35, pp. 18–31, 2017.<br />
[2] Steven E. Underwood, Road Vehicle Automation, 2014<br />
[3] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia, “Pyramid scene parsing network,”
Proceedings - 30th IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2017, vol. 2017-
January, pp. 6230–6239, 2017.<br />
[4] Zifeng Wu, Chunhua Shen, and Anton van den Hen-
gel, “Wider or Deeper: Revisiting the ResNet Model
for Visual Recognition,” Pattern Recognition, vol. 90,
no. November 2016, pp. 119–133, 2019.<br />
[5] Liang Chieh Chen, Yukun Zhu, George Papandreou,
Florian Schroff, and Hartwig Adam, “Encoder-decoder
with atrous separable convolution for semantic image
segmentation,” Lecture Notes in Computer Science (in-
cluding subseries Lecture Notes in Artificial Intelligence
and Lecture Notes in Bioinformatics), vol. 11211 LNCS,
pp. 833–851, 2018.</p>


		
	</div>

	<div class="pagination">
		<a href="/posts/fintec_ki/" class="left arrow">&#8592;</a>
		<a href="/posts/gitlab-shell-deployment/" class="right arrow">&#8594;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			
			<span>
			&copy; <time datetime="2021-02-17 14:12:03.726787404 &#43;0100 CET m=&#43;0.082553366">2021</time> . Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
